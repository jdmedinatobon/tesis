{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PruebaColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQZMEpmyE5eWU2Na8Mx4Dw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdmedinatobon/tesis/blob/master/PruebaColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvZhGOdLVPe",
        "colab_type": "text"
      },
      "source": [
        "#Instalación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbd2RLxmLHqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "ee34b797-29a6-40b6-b562-1f0c8046daba"
      },
      "source": [
        "#Correr esta celda si se utiliza Google Colab con un ambiente remoto.\n",
        "!pip install progressbar2\n",
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (3.38.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2) (1.12.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpB5mKCoLaVi",
        "colab_type": "text"
      },
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqXrBCcMLX1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import progressbar\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9xdjldULdOt",
        "colab_type": "text"
      },
      "source": [
        "#Clase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhcA0FR4Leek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExperienceReplay():\n",
        "    def __init__(self, tamano_minimo = 1000, tamano_maximo = 10**6, tamano_batch = 32):\n",
        "        self.tamano_buffer = 0\n",
        "        self.tamano_batch = tamano_batch\n",
        "        self.tamano_minimo = tamano_minimo\n",
        "        self.tamano_maximo = tamano_maximo\n",
        "        self.buffer = np.empty(1)\n",
        "        \n",
        "    def agregarDato(self, x, a, r, xp):\n",
        "        dato = dict({'St':x, 'At':a, 'Rt+1':r, 'St+1':xp})\n",
        "        \n",
        "        if(self.tamano_buffer == 0):\n",
        "            self.buffer = np.append(self.buffer, dato)\n",
        "            self.buffer = np.delete(self.buffer, 0)\n",
        "            self.tamano_buffer = 1\n",
        "        else:\n",
        "            self.buffer = np.append(self.buffer, dato)\n",
        "            self.tamano_buffer += 1\n",
        "        \n",
        "        if(self.tamano_buffer > self.tamano_maximo):\n",
        "            self.buffer = np.delete(self.buffer, 0)\n",
        "            self.tamano_buffer -= 1\n",
        "            \n",
        "    \n",
        "    def darMuestras(self):\n",
        "        indices = np.random.choice(range(self.tamano_buffer), size = self.tamano_batch, replace = False)\n",
        "        return self.buffer[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fKp_90xLgD9",
        "colab_type": "text"
      },
      "source": [
        "#Table Lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZgJsQDLiy9",
        "colab_type": "text"
      },
      "source": [
        "Pendulum-v0<br /> \n",
        "Utilizando el algoritmo de Sarsa.<br />\n",
        "Está como chambón pero funciona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TISfAzCFLia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def darIndice(pS):\n",
        "    return int(round(pS, 1)*10+10)\n",
        "\n",
        "def escogerAccion(pS1, pS2, pS3):\n",
        "    num = np.random.random()\n",
        "    \n",
        "    if(num <= epsilon):\n",
        "        ind = np.random.randint(0,41)\n",
        "        return ind\n",
        "    else:\n",
        "        ind = np.where(tablaQ[pS1, pS2, pS3, :] == np.amax(tablaQ[pS1, pS2, pS3, :]))\n",
        "        ind = ind[0]\n",
        "        \n",
        "        if(len(ind) > 1):\n",
        "            return np.random.choice(ind)\n",
        "        else:\n",
        "            return ind\n",
        "    \n",
        "    return accion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5OJPXA9LnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "envName = 'Pendulum-v0'\n",
        "\n",
        "g = 0.9\n",
        "alpha = 0.5\n",
        "epsilon = 0.1 #Entre 0 y 1\n",
        "maxIter = 1500000\n",
        "ventana = 100\n",
        "epsilonIterLimit = maxIter*0.9\n",
        "\n",
        "tablaQ = np.ones((21,21,17,41))*-16\n",
        "rewards = np.zeros(int(maxIter/10))\n",
        "\n",
        "env = gym.make(envName)\n",
        "observation = env.reset()\n",
        "\n",
        "s1 = darIndice(observation[0])\n",
        "s2 = darIndice(observation[1])\n",
        "s3 = int(round(observation[2], 0) + 8)\n",
        "a = escogerAccion(s1, s2, s3)\n",
        "\n",
        "suma = 0\n",
        "\n",
        "for i in progressbar.progressbar(range(maxIter)):\n",
        "    \n",
        "    if(i > maxIter-2000):\n",
        "        env.render()\n",
        "    \n",
        "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
        "    observation = np.reshape(observation, (3,))\n",
        "    \n",
        "    s1p = darIndice(observation[0])\n",
        "    s2p = darIndice(observation[1])\n",
        "    s3p = int(round(observation[2], 0) + 8)\n",
        "    \n",
        "    ap = escogerAccion(s1p, s2p, s3p)\n",
        "    \n",
        "    r = reward #Por ahora\n",
        "    suma+=r\n",
        "    \n",
        "    if(i % ventana == 0):\n",
        "        rewards[int(i/ventana)] = suma/ventana\n",
        "        suma = 0\n",
        "    \n",
        "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
        "    \n",
        "    s1 = s1p\n",
        "    s2 = s2p\n",
        "    s3 = s3p\n",
        "    a = ap\n",
        "    \n",
        "    if(i > epsilonIterLimit):\n",
        "        epsilon = 0\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xryqaA7LpWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in progressbar.progressbar(range(2000)):\n",
        "    env.render()\n",
        "    \n",
        "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
        "    observation = np.reshape(observation, (3,))\n",
        "    \n",
        "    s1p = darIndice(observation[0])\n",
        "    s2p = darIndice(observation[1])\n",
        "    s3p = int(round(observation[2], 0) + 8)\n",
        "    \n",
        "    ap = escogerAccion(s1p, s2p, s3p)\n",
        "    \n",
        "    r = reward #Por ahora\n",
        "    suma+=r\n",
        "    \n",
        "    if(i % ventana == 0):\n",
        "        rewards[int(i/ventana)] = suma/ventana\n",
        "        suma = 0\n",
        "    \n",
        "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
        "    \n",
        "    s1 = s1p\n",
        "    s2 = s2p\n",
        "    s3 = s3p\n",
        "    a = ap\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euEc1_rlLqdQ",
        "colab_type": "text"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSus9n4dLrcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creando redes neuronales\n",
        "def crearRedes(n1 = 20, n2 = 41, learning_rate = 0.01):\n",
        "    init = RandomNormal(seed = 0)\n",
        "    \n",
        "    modelo = Sequential()\n",
        "    \n",
        "    modelo.add(Dense(n1, activation = 'relu', kernel_initializer = init, bias_initializer = init, input_dim = 3))\n",
        "    modelo.add(Dense(n2, activation = 'linear', kernel_initializer = init, bias_initializer = init))\n",
        "    \n",
        "    sgd = optimizers.sgd(lr = learning_rate)\n",
        "    \n",
        "    modelo.compile(loss = 'mean_squared_error', optimizer = sgd, metrics = ['accuracy'])\n",
        "    \n",
        "    return modelo, copy.deepcopy(modelo)\n",
        "\n",
        "def actualizarRedTarget(pRedPrincipal, pRedTarget):\n",
        "    \n",
        "    for i in range(len(pRedPrincipal.layers)):\n",
        "        pesosPrincipal = pRedPrincipal.layers[i].get_weights()\n",
        "        pesosTarget = pRedTarget.layers[i].get_weights()\n",
        "        \n",
        "        pesos = [tao*l1 + (1-tao)*l2 for l1, l2 in zip(pesosPrincipal, pesosTarget)]\n",
        "        \n",
        "        pRedTarget.layers[i].set_weights(pesos)\n",
        "\n",
        "def escogerAccion(pModelo, pX):\n",
        "    num = np.random.random()\n",
        "    \n",
        "    if(num <= epsilon):\n",
        "        ind = np.random.randint(0,41)\n",
        "        return ind\n",
        "    else:\n",
        "        ind = np.argmax(pModelo.predict(pX))\n",
        "        return ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi08x7A1LtZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "envName = 'Pendulum-v0'\n",
        "\n",
        "modeloPrincipal, modeloTarget = crearRedes(learning_rate = 0.01)\n",
        "\n",
        "#Con los parametros default.\n",
        "buffer = ExperienceReplay()\n",
        "\n",
        "gamma = 0.99\n",
        "alpha = 0.35\n",
        "epsilon = 1#Entre 0 y 1\n",
        "epsilonDecay = 0.99\n",
        "tao = 0.001\n",
        "\n",
        "episodes = 100\n",
        "stepsPrueba = 500\n",
        "maxSteps = 100\n",
        "contadores = np.zeros((1,41))\n",
        "\n",
        "rewardsAcumulados = np.zeros(episodes)\n",
        "\n",
        "env = gym.make(envName)\n",
        "observation = env.reset()\n",
        "\n",
        "x = np.reshape(observation, (1,3))\n",
        "x[0,2] = x[0,2]/8.0\n",
        "\n",
        "a = escogerAccion(modeloPrincipal, x)\n",
        "\n",
        "#Este es un for inicial donde se almacena la cantidad minima de datos requerida en el buffer\n",
        "#antes de comenzar con el entrenamiento.\n",
        "for i in progressbar.progressbar(range(buffer.tamano_minimo)):\n",
        "    observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
        "    \n",
        "    xp = np.reshape(observation, (1,3))\n",
        "    xp[0,2] = xp[0,2]/8.0\n",
        "    x = xp\n",
        "    \n",
        "    buffer.agregarDato(x, a, reward, xp)\n",
        "    \n",
        "    #epsilon es 1 asi que siempre va a escoger al azar.\n",
        "    a = escogerAccion(modeloPrincipal, x)\n",
        "    \n",
        "for e in progressbar.progressbar(range(episodes)):\n",
        "    rewardAcumulado = 0\n",
        "    observation = env.reset()\n",
        "    x = np.reshape(observation, (1,3))\n",
        "    x[0,2] = x[0,2]/8.0\n",
        "    a = escogerAccion(modeloPrincipal, x)\n",
        "    \n",
        "    for s in range(maxSteps):\n",
        "        observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
        "        \n",
        "        rewardAcumulado += reward\n",
        "        \n",
        "        xp = observation\n",
        "        xp = np.reshape(xp, (1,3))\n",
        "        xp[0,2] = xp[0,2]/8.0\n",
        "\n",
        "        buffer.agregarDato(x, a, reward, xp)\n",
        "\n",
        "        muestras = buffer.darMuestras()\n",
        "\n",
        "        #Pasar esto a un metodo solito\n",
        "        xEnt = np.empty((1,3))\n",
        "        aEnt = np.empty(1)\n",
        "        rEnt = np.empty(1)\n",
        "        xpEnt = np.empty((1,3))\n",
        "\n",
        "        for m in muestras:\n",
        "            xEnt = np.append(xEnt, m.get('St'), axis = 0)\n",
        "            aEnt = np.append(aEnt, [m.get('At')], axis = 0)\n",
        "            rEnt = np.append(rEnt, [m.get('Rt+1')], axis = 0)\n",
        "            xpEnt = np.append(xpEnt, m.get('St+1'), axis = 0)\n",
        "\n",
        "        xEnt = np.delete(xEnt, 0, axis = 0)\n",
        "        aEnt = np.delete(aEnt, 0, axis = 0)\n",
        "        rEnt = np.delete(rEnt, 0, axis = 0)\n",
        "        xpEnt = np.delete(xpEnt, 0, axis = 0)\n",
        "\n",
        "        qValores = modeloPrincipal.predict(xEnt)\n",
        "\n",
        "        qValoresTarget = modeloTarget.predict(xEnt)\n",
        "        maxQvalores = np.amax(qValoresTarget, axis = 1)\n",
        "\n",
        "        #Aqui hay que calcular el maximo\n",
        "        targets = np.add(rEnt, gamma*maxQvalores)\n",
        "\n",
        "        yEnt = qValores\n",
        "\n",
        "        for j in range(buffer.tamano_batch):\n",
        "            yEnt[j,int(aEnt[j])] = targets[j]\n",
        "\n",
        "        modeloPrincipal.train_on_batch(xEnt, yEnt)\n",
        "        actualizarRedTarget(modeloPrincipal, modeloTarget)\n",
        "\n",
        "        x = xp\n",
        "        a = escogerAccion(modeloPrincipal, x)\n",
        "\n",
        "    epsilon = epsilon*epsilonDecay\n",
        "    rewardsAcumulados[e] = rewardAcumulado\n",
        "\n",
        "observation = env.reset()\n",
        "x = np.reshape(observation, (1,3))\n",
        "x[0,2] = x[0,2]/8.0\n",
        "a = escogerAccion(modeloPrincipal, x)    \n",
        "    \n",
        "for s in range(stepsPrueba):\n",
        "    \n",
        "    observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    xp = observation\n",
        "    xp = np.reshape(xp, (1,3))\n",
        "    xp[0,2] = xp[0,2]/8.0\n",
        "    \n",
        "    a = escogerAccion(modeloPrincipal, xp)\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_7xGlR6LvLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(rewardsAcumulados)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward Acumulado')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}