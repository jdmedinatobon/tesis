{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este archivo es para implementar algunos algoritmos de aprendizaje por refuerzo y aplicarlos utilizando Open AI gym.\n",
    "\n",
    "Se van a probar los siguientes 2 entornos (environments), de acuerdo con los resultados presentados en el articulo de Diego:\n",
    "\n",
    "Pendulum-v0 y Hopper-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tareas (Puede cambiar):\n",
    "1. Terminar el curso de David Silver. (Completo: 100%)\n",
    "2. Implementar con table lookup. (Completo: 100%)\n",
    "3. Implementar con DQN. (En progreso: Aun no funciona)\n",
    "4. Implementar con Simglucose (Sin comenzar)\n",
    "5. Implementar con el trabajo de Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar2 in /home/hero/tf/lib/python3.6/site-packages (3.47.0)\n",
      "Requirement already satisfied: six in /home/hero/tf/lib/python3.6/site-packages (from progressbar2) (1.14.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /home/hero/tf/lib/python3.6/site-packages (from progressbar2) (2.3.0)\n",
      "Requirement already satisfied: gym in /home/hero/gym (0.15.4)\n",
      "Requirement already satisfied: scipy in /home/hero/tf/lib/python3.6/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/hero/tf/lib/python3.6/site-packages (from gym) (1.18.1)\n",
      "Requirement already satisfied: six in /home/hero/tf/lib/python3.6/site-packages (from gym) (1.14.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /home/hero/tf/lib/python3.6/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/hero/tf/lib/python3.6/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: future in /home/hero/tf/lib/python3.6/site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "#Correr esta celda si se utiliza Google Colab con un ambiente remoto.\n",
    "!pip install progressbar2\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time, copy, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import progressbar\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, tamano_minimo = 1000, tamano_maximo = 10**5, tamano_batch = 32, column_names = ['St', 'At', 'St+1', 'Rt+1', 'done']):\n",
    "        self.tamano_buffer = 0\n",
    "        self.tamano_batch = tamano_batch\n",
    "        self.tamano_minimo = tamano_minimo\n",
    "        self.tamano_maximo = tamano_maximo\n",
    "        self.columnas = column_names\n",
    "        \n",
    "        self.buffer = self.inicializarBuffer()\n",
    "        self.buffer.columns = column_names\n",
    "        #self.indices = range(self.tamano_maximo)\n",
    "    \n",
    "    def inicializarBuffer(self):\n",
    "        aux = pd.DataFrame(np.zeros((10**4,5)), dtype = 'object')\n",
    "        #aux = pd.DataFrame({'St': None, 'At': [np.array([0,0])],\n",
    "        #                   'Rt+1': [np.array([0,0])], 'St+1': [np.array([0,0])],\n",
    "        #                   'done': [np.array([0,0])]})\n",
    "        \n",
    "        buffer = []\n",
    "        \n",
    "        for i in range(int(self.tamano_maximo/10**4)):\n",
    "            buffer.append(aux)\n",
    "        \n",
    "        return pd.concat(buffer)\n",
    "    \n",
    "    def agregarDato(self, x, a, r, xp, d):\n",
    "        \n",
    "        self.buffer.iloc[self.tamano_buffer]['St'] = x\n",
    "        self.buffer.iloc[self.tamano_buffer]['At'] = a\n",
    "        self.buffer.iloc[self.tamano_buffer]['Rt+1'] = r\n",
    "        self.buffer.iloc[self.tamano_buffer]['St+1'] = xp\n",
    "        self.buffer.iloc[self.tamano_buffer]['done'] = d\n",
    "                       \n",
    "        self.tamano_buffer +=1\n",
    "                       \n",
    "        if(self.tamano_buffer == self.tamano_maximo):\n",
    "            self.tamano_buffer = 0\n",
    "    \n",
    "    def darMuestras(self):\n",
    "        #indices = random.sample(self.indices[0:self.tamano_buffer], self.tamano_batch)\n",
    "        indices = random.sample(range(self.tamano_buffer), self.tamano_batch)\n",
    "        return self.buffer.iloc[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum-v0<br /> \n",
    "Utilizando el algoritmo de Sarsa.<br />\n",
    "Está como chambón pero funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darIndice(pS):\n",
    "    return int(round(pS, 1)*10+10)\n",
    "\n",
    "def escogerAccion(pS1, pS2, pS3):\n",
    "    num = np.random.random()\n",
    "    \n",
    "    if(num <= epsilon):\n",
    "        ind = np.random.randint(0,41)\n",
    "        return ind\n",
    "    else:\n",
    "        ind = np.where(tablaQ[pS1, pS2, pS3, :] == np.amax(tablaQ[pS1, pS2, pS3, :]))\n",
    "        ind = ind[0]\n",
    "        \n",
    "        if(len(ind) > 1):\n",
    "            return np.random.choice(ind)\n",
    "        else:\n",
    "            return ind\n",
    "    \n",
    "    return accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envName = 'Pendulum-v0'\n",
    "\n",
    "g = 0.9\n",
    "alpha = 0.5\n",
    "epsilon = 0.1 #Entre 0 y 1\n",
    "maxIter = 1500000\n",
    "ventana = 100\n",
    "epsilonIterLimit = maxIter*0.9\n",
    "\n",
    "tablaQ = np.ones((21,21,17,41))*-16\n",
    "rewards = np.zeros(int(maxIter/10))\n",
    "\n",
    "env = gym.make(envName)\n",
    "observation = env.reset()\n",
    "\n",
    "s1 = darIndice(observation[0])\n",
    "s2 = darIndice(observation[1])\n",
    "s3 = int(round(observation[2], 0) + 8)\n",
    "a = escogerAccion(s1, s2, s3)\n",
    "\n",
    "suma = 0\n",
    "\n",
    "for i in progressbar.progressbar(range(maxIter)):\n",
    "    \n",
    "    if(i > maxIter-2000):\n",
    "        env.render()\n",
    "    \n",
    "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
    "    observation = np.reshape(observation, (3,))\n",
    "    \n",
    "    s1p = darIndice(observation[0])\n",
    "    s2p = darIndice(observation[1])\n",
    "    s3p = int(round(observation[2], 0) + 8)\n",
    "    \n",
    "    ap = escogerAccion(s1p, s2p, s3p)\n",
    "    \n",
    "    r = reward #Por ahora\n",
    "    suma+=r\n",
    "    \n",
    "    if(i % ventana == 0):\n",
    "        rewards[int(i/ventana)] = suma/ventana\n",
    "        suma = 0\n",
    "    \n",
    "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
    "    \n",
    "    s1 = s1p\n",
    "    s2 = s2p\n",
    "    s3 = s3p\n",
    "    a = ap\n",
    "    \n",
    "    if(i > epsilonIterLimit):\n",
    "        epsilon = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in progressbar.progressbar(range(2000)):\n",
    "    env.render()\n",
    "    \n",
    "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
    "    observation = np.reshape(observation, (3,))\n",
    "    \n",
    "    s1p = darIndice(observation[0])\n",
    "    s2p = darIndice(observation[1])\n",
    "    s3p = int(round(observation[2], 0) + 8)\n",
    "    \n",
    "    ap = escogerAccion(s1p, s2p, s3p)\n",
    "    \n",
    "    r = reward #Por ahora\n",
    "    suma+=r\n",
    "    \n",
    "    if(i % ventana == 0):\n",
    "        rewards[int(i/ventana)] = suma/ventana\n",
    "        suma = 0\n",
    "    \n",
    "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
    "    \n",
    "    s1 = s1p\n",
    "    s2 = s2p\n",
    "    s3 = s3p\n",
    "    a = ap\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hero/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "envName = 'Pendulum-v0'\n",
    "env = gym.make(envName) \n",
    "\n",
    "num_acciones = 41\n",
    "dim_state = 3\n",
    "\n",
    "#Creando redes neuronales\n",
    "def crearRedes(n1 = 20, n2 = num_acciones, learning_rate = 0.01):\n",
    "    init = RandomNormal(seed = 0)\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    \n",
    "    modelo.add(Dense(n1, activation = 'relu', kernel_initializer = init, bias_initializer = init, input_dim = dim_state))\n",
    "    modelo.add(Dense(n1, activation = 'relu', kernel_initializer = init, bias_initializer = init))\n",
    "    modelo.add(Dense(n2, activation = 'linear', kernel_initializer = init, bias_initializer = init))\n",
    "    \n",
    "    sgd = optimizers.sgd(lr = learning_rate)\n",
    "    \n",
    "    modelo.compile(loss = 'mean_squared_error', optimizer = sgd, metrics = ['accuracy'])\n",
    "    \n",
    "    return modelo, copy.deepcopy(modelo)\n",
    "\n",
    "def actualizarRedTarget(pRedPrincipal, pRedTarget):\n",
    "    \n",
    "    for i in range(len(pRedPrincipal.layers)):\n",
    "        pesosPrincipal = pRedPrincipal.layers[i].get_weights()\n",
    "        pesosTarget = pRedTarget.layers[i].get_weights()\n",
    "        \n",
    "        pesos = [tao*l1 + (1-tao)*l2 for l1, l2 in zip(pesosPrincipal, pesosTarget)]\n",
    "        \n",
    "        pRedTarget.layers[i].set_weights(pesos)\n",
    "\n",
    "def escogerAccion(pModelo, pX):\n",
    "    num = np.random.random()\n",
    "    \n",
    "    if(num <= epsilon):\n",
    "        ind = np.random.randint(0, num_acciones)\n",
    "        return ind\n",
    "    else:\n",
    "        ind = np.argmax(pModelo.predict(pX))\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (500 of 500) |######################| Elapsed Time: 0:05:16 Time:  0:05:16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-766cce683379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_acumulados\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subplot' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(123)       # For reproducibility\n",
    "\n",
    "buffer = ExperienceReplay() # Instance of the replay_buffer (to record experiences)\n",
    "\n",
    "# Create an instance of the neural network\n",
    "red, red_target = crearRedes(learning_rate = 0.05)\n",
    "\n",
    "num_episodes = 500             # Number of training episodes\n",
    "num_steps = 100                 # Number of steps per episode\n",
    "epsilon = 1.0                   # Initial exploration probability\n",
    "tao = 0.001                     # Soft-update learning rate\n",
    "tau = tao\n",
    "gamma = 0.99                    # Discount factor\n",
    "batch_size = 32                 # Batch size\n",
    "\n",
    "total_steps_log = []            # Empty list to log number of steps per episode \n",
    "epsilon_log = []                # Empty list to log exploration probabilities\n",
    "muestras_log = []\n",
    "rewards_acumulados = []\n",
    "\n",
    "initial_time = time.time()      # For comparison purposes\n",
    "\n",
    "for e in progressbar.progressbar(range(num_episodes)):   # Loop over episodes\n",
    "    state = env.reset()         # Reset environment\n",
    "    state[2] *= 1/8.0\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    \n",
    "    rewardAcumulado = 0\n",
    "    \n",
    "    for s in range(num_steps):  # Loop over steps\n",
    "        if(np.random.random() <= epsilon): # epsilon-greedy policy                    \n",
    "            action = np.random.randint(num_acciones)\n",
    "        else:\n",
    "            action = np.argmax(red.predict(state))\n",
    "            \n",
    "        next_state, reward, done, info = env.step([(action-20.0)/10.0])  # Execute selected action\n",
    "        next_state[2] *= 1/8.0\n",
    "        next_state = np.reshape(next_state, (1, -1))\n",
    "        \n",
    "        buffer.agregarDato(state, action, reward, next_state, done)\n",
    "       \n",
    "        state = next_state   # Update state\n",
    "        \n",
    "        sumaMuestras = 0\n",
    "        \n",
    "        if(buffer.tamano_buffer >= buffer.tamano_minimo):   # Neural network training\n",
    "            tm = time.time()\n",
    "            muestras = buffer.darMuestras() # Sample a batch of experiences\n",
    "            sumaMuestras += time.time()-tm\n",
    "            \n",
    "            states = np.concatenate(muestras['St'].values)\n",
    "            actions = muestras['At'].values\n",
    "            rewards = muestras['Rt+1'].values\n",
    "            next_states = np.concatenate(muestras['St+1'].values)\n",
    "            dones = muestras['done'].values\n",
    "            \n",
    "            # Forward pass over the DQN\n",
    "            Q_values = red.predict(states)\n",
    "            \n",
    "            # Forward pass over the target DQN\n",
    "            next_Q_values = red_target.predict(next_states)\n",
    "            targets = Q_values.copy()   # Set initial target values\n",
    "\n",
    "            targets[np.arange(buffer.tamano_batch), list(actions)] = rewards + gamma* np.max(next_Q_values, axis = 1) * (1 - dones)\n",
    "            \n",
    "            red.train_on_batch(states, targets)\n",
    "            \n",
    "            actualizarRedTarget(red, red_target)\n",
    "        \n",
    "        rewardAcumulado += reward\n",
    "        \n",
    "        if(done): # Termination condition\n",
    "            break    \n",
    "    \n",
    "    sumaMuestras *= 1/float(s)\n",
    "    muestras_log.append(sumaMuestras)\n",
    "    rewards_acumulados.append(rewardAcumulado)\n",
    "    \n",
    "    total_steps_log.append(s)     # Append total number of steps\n",
    "    epsilon_log.append(epsilon)   # Append exploration probability \n",
    "    epsilon = 0.995 * epsilon     # Exploration decay\n",
    "\n",
    "total_training_time = time.time() - initial_time # Total training time    \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(rewards_acumulados)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Episode')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(muestras_log)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sample time')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(epsilon_log)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "\n",
    "print('Total training time: ', np.round(total_training_time, 2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "steps_prueba = 1000\n",
    "\n",
    "state = env.reset()\n",
    "state[2] *= 1/8.0\n",
    "state = np.reshape(state, (1, -1))\n",
    "\n",
    "for s in range(steps_prueba):\n",
    "    \n",
    "    action = np.argmax(red.predict(state))\n",
    "    state, reward, done, info = env.step([(action-10.0)/20.0])\n",
    "    \n",
    "    state[2] *= 1/8.0\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 41)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creando redes neuronales\n",
    "def crearRedes(n1 = 20, n2 = 4, learning_rate = 0.01):\n",
    "    init = RandomNormal(seed = 0)\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    \n",
    "    modelo.add(Dense(n1, activation = 'relu', kernel_initializer = init, bias_initializer = init, input_dim = 3))\n",
    "    modelo.add(Dense(n2, activation = 'linear', kernel_initializer = init, bias_initializer = init))\n",
    "    \n",
    "    sgd = optimizers.sgd(lr = learning_rate)\n",
    "    \n",
    "    modelo.compile(loss = 'mean_squared_error', optimizer = sgd, metrics = ['accuracy'])\n",
    "    \n",
    "    return modelo, copy.deepcopy(modelo)\n",
    "\n",
    "def actualizarRedTarget(pRedPrincipal, pRedTarget):\n",
    "    \n",
    "    for i in range(len(pRedPrincipal.layers)):\n",
    "        pesosPrincipal = pRedPrincipal.layers[i].get_weights()\n",
    "        pesosTarget = pRedTarget.layers[i].get_weights()\n",
    "        \n",
    "        pesos = [tao*l1 + (1-tao)*l2 for l1, l2 in zip(pesosPrincipal, pesosTarget)]\n",
    "        \n",
    "        pRedTarget.layers[i].set_weights(pesos)\n",
    "\n",
    "def escogerAccion(pModelo, pX):\n",
    "    num = np.random.random()\n",
    "    \n",
    "    if(num <= epsilon):\n",
    "        ind = np.random.randint(0,41)\n",
    "        return ind\n",
    "    else:\n",
    "        ind = np.argmax(pModelo.predict(pX))\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Grid_World()      \n",
    "\n",
    "modeloPrincipal, modeloTarget = crearRedes(learning_rate = 0.01)\n",
    "\n",
    "#Con los parametros default.\n",
    "buffer = ExperienceReplay()\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.35\n",
    "epsilon = 1#Entre 0 y 1\n",
    "epsilonDecay = 0.99\n",
    "tao = 0.001\n",
    "\n",
    "episodes = 1000\n",
    "stepsPrueba = 500\n",
    "maxSteps = 10\n",
    "contadores = np.zeros((1,41))\n",
    "\n",
    "rewardsAcumulados = np.zeros(episodes)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "x = np.reshape(observation, (1,3))\n",
    "x[0,2] = x[0,2]/8.0\n",
    "\n",
    "a = escogerAccion(modeloPrincipal, x)\n",
    "\n",
    "#Este es un for inicial donde se almacena la cantidad minima de datos requerida en el buffer\n",
    "#antes de comenzar con el entrenamiento.\n",
    "for i in progressbar.progressbar(range(buffer.tamano_minimo)):\n",
    "    observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
    "    \n",
    "    xp = np.reshape(observation, (1,3))\n",
    "    xp[0,2] = xp[0,2]/8.0\n",
    "    x = xp\n",
    "    \n",
    "    buffer.agregarDato(x, a, reward, xp)\n",
    "    \n",
    "    #epsilon es 1 asi que siempre va a escoger al azar.\n",
    "    a = escogerAccion(modeloPrincipal, x)\n",
    "    \n",
    "for e in progressbar.progressbar(range(episodes)):\n",
    "    rewardAcumulado = 0\n",
    "    env = gym.make(envName)\n",
    "    observation = env.reset()\n",
    "    x = np.reshape(observation, (1,3))\n",
    "    x[0,2] = x[0,2]/8.0\n",
    "    a = escogerAccion(modeloPrincipal, x)\n",
    "    \n",
    "    for s in range(maxSteps):\n",
    "        observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
    "        \n",
    "        rewardAcumulado += reward\n",
    "        \n",
    "        xp = observation\n",
    "        xp = np.reshape(xp, (1,3))\n",
    "        xp[0,2] = xp[0,2]/8.0\n",
    "\n",
    "        buffer.agregarDato(x, a, reward, xp)\n",
    "\n",
    "        muestras = buffer.darMuestras()\n",
    "\n",
    "        #Pasar esto a un metodo solito\n",
    "        xEnt = np.empty((1,3))\n",
    "        aEnt = np.empty(1)\n",
    "        rEnt = np.empty(1)\n",
    "        xpEnt = np.empty((1,3))\n",
    "\n",
    "        for m in muestras:\n",
    "            xEnt = np.append(xEnt, m.get('St'), axis = 0)\n",
    "            aEnt = np.append(aEnt, [m.get('At')], axis = 0)\n",
    "            rEnt = np.append(rEnt, [m.get('Rt+1')], axis = 0)\n",
    "            xpEnt = np.append(xpEnt, m.get('St+1'), axis = 0)\n",
    "\n",
    "        xEnt = np.delete(xEnt, 0, axis = 0)\n",
    "        aEnt = np.delete(aEnt, 0, axis = 0)\n",
    "        rEnt = np.delete(rEnt, 0, axis = 0)\n",
    "        xpEnt = np.delete(xpEnt, 0, axis = 0)\n",
    "\n",
    "        qValores = modeloPrincipal.predict(xEnt)\n",
    "\n",
    "        qValoresTarget = modeloTarget.predict(xEnt)\n",
    "        maxQvalores = np.amax(qValoresTarget, axis = 1)\n",
    "\n",
    "        #Aqui hay que calcular el maximo\n",
    "        targets = np.add(rEnt, gamma*maxQvalores)\n",
    "\n",
    "        yEnt = qValores\n",
    "\n",
    "        for j in range(buffer.tamano_batch):\n",
    "            yEnt[j,int(aEnt[j])] = targets[j]\n",
    "\n",
    "        modeloPrincipal.fit(xEnt, yEnt, verbose = 0)\n",
    "        actualizarRedTarget(modeloPrincipal, modeloTarget)\n",
    "\n",
    "        x = xp\n",
    "        a = escogerAccion(modeloPrincipal, x)\n",
    "\n",
    "    epsilon = epsilon*epsilonDecay\n",
    "    rewardsAcumulados[e] = rewardAcumulado\n",
    "\n",
    "env = gym.make(envName)\n",
    "observation = env.reset()\n",
    "x = np.reshape(observation, (1,3))\n",
    "x[0,2] = x[0,2]/8.0\n",
    "a = escogerAccion(modeloPrincipal, x)    \n",
    "    \n",
    "for s in range(stepsPrueba):\n",
    "    \n",
    "    observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    xp = observation\n",
    "    xp = np.reshape(xp, (1,3))\n",
    "    xp[0,2] = xp[0,2]/8.0\n",
    "    \n",
    "    a = escogerAccion(modeloPrincipal, xp)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.317124741065786e-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.99**1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aEnt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 618.,  266.,  253.,  333., 1241.,  345.,  308.,  830., 1303.,\n",
       "         797.,  452.,  332.,  983.,  149.,  362.,  178., 3307.,  268.,\n",
       "         438., 1041.,  542.,  413.,  586.,  630.,  830.,  494.,  925.,\n",
       "        1385., 1835.,  428.,  368.,  713.,  669.,  751., 1593.,  508.,\n",
       "         770., 1066.,  255., 1183.,  252.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006736262610603238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9999**50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([10,22,50,-1,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = np.where(test == np.amax(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[10/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.tamano_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
