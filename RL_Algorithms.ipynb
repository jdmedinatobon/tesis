{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este archivo es para implementar algunos algoritmos de aprendizaje por refuerzo y aplicarlos utilizando Open AI gym.\n",
    "\n",
    "Se van a probar los siguientes 2 entornos (environments), de acuerdo con los resultados presentados en el articulo de Diego:\n",
    "\n",
    "Pendulum-v0 y Hopper-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tareas (Puede cambiar):\n",
    "1. Terminar el curso de David Silver. (Completo: 100%)\n",
    "2. Implementar con table lookup. (En progreso: Ya medio funciona (por ahi un 80%))\n",
    "3. Implementar con DQN. (Sin comenzar)\n",
    "4. Implementar con Simglucose (Sin comenzar)\n",
    "5. Implementar con el trabajo de Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import progressbar\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum-v0<br /> \n",
    "Utilizando el algoritmo de Sarsa.<br />\n",
    "Está como chambón pero funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darIndice(pS):\n",
    "    return int(round(pS, 1)*10+10)\n",
    "\n",
    "def escogerAccion(pS1, pS2, pS3):\n",
    "    num = np.random.random()\n",
    "    \n",
    "    if(num <= epsilon):\n",
    "        ind = np.random.randint(0,41)\n",
    "        return ind\n",
    "    else:\n",
    "        ind = np.where(tablaQ[pS1, pS2, pS3, :] == np.amax(tablaQ[pS1, pS2, pS3, :]))\n",
    "        ind = ind[0]\n",
    "        \n",
    "        if(len(ind) > 1):\n",
    "            return np.random.choice(ind)\n",
    "        else:\n",
    "            return ind\n",
    "    \n",
    "    return accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envName = 'Pendulum-v0'\n",
    "\n",
    "g = 0.9\n",
    "alpha = 0.5\n",
    "epsilon = 0.1 #Entre 0 y 1\n",
    "maxIter = 1500000\n",
    "ventana = 100\n",
    "epsilonIterLimit = maxIter*0.9\n",
    "\n",
    "tablaQ = np.ones((21,21,17,41))*-16\n",
    "rewards = np.zeros(int(maxIter/10))\n",
    "\n",
    "env = gym.make(envName)\n",
    "observation = env.reset()\n",
    "\n",
    "s1 = darIndice(observation[0])\n",
    "s2 = darIndice(observation[1])\n",
    "s3 = int(round(observation[2], 0) + 8)\n",
    "a = escogerAccion(s1, s2, s3)\n",
    "\n",
    "suma = 0\n",
    "\n",
    "for i in progressbar.progressbar(range(maxIter)):\n",
    "    \n",
    "    if(i > maxIter-2000):\n",
    "        env.render()\n",
    "    \n",
    "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
    "    observation = np.reshape(observation, (3,))\n",
    "    \n",
    "    s1p = darIndice(observation[0])\n",
    "    s2p = darIndice(observation[1])\n",
    "    s3p = int(round(observation[2], 0) + 8)\n",
    "    \n",
    "    ap = escogerAccion(s1p, s2p, s3p)\n",
    "    \n",
    "    r = reward #Por ahora\n",
    "    suma+=r\n",
    "    \n",
    "    if(i % ventana == 0):\n",
    "        rewards[int(i/ventana)] = suma/ventana\n",
    "        suma = 0\n",
    "    \n",
    "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
    "    \n",
    "    s1 = s1p\n",
    "    s2 = s2p\n",
    "    s3 = s3p\n",
    "    a = ap\n",
    "    \n",
    "    if(i > epsilonIterLimit):\n",
    "        epsilon = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in progressbar.progressbar(range(2000)):\n",
    "    env.render()\n",
    "    \n",
    "    observation, reward, done, info = env.step([(a-20)/10.0])\n",
    "    observation = np.reshape(observation, (3,))\n",
    "    \n",
    "    s1p = darIndice(observation[0])\n",
    "    s2p = darIndice(observation[1])\n",
    "    s3p = int(round(observation[2], 0) + 8)\n",
    "    \n",
    "    ap = escogerAccion(s1p, s2p, s3p)\n",
    "    \n",
    "    r = reward #Por ahora\n",
    "    suma+=r\n",
    "    \n",
    "    if(i % ventana == 0):\n",
    "        rewards[int(i/ventana)] = suma/ventana\n",
    "        suma = 0\n",
    "    \n",
    "    tablaQ[s1, s2, s3, a] = tablaQ[s1, s2, s3, a] + alpha*(r + g*tablaQ[s1p, s2p, s3p, ap] - tablaQ[s1,s2,s3,a])\n",
    "    \n",
    "    s1 = s1p\n",
    "    s2 = s2p\n",
    "    s3 = s3p\n",
    "    a = ap\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando red neuronal\n",
    "def crearModelo(n1 = 40, n2 = 41, learning_rate = 0.01):\n",
    "    modelo = Sequential()\n",
    "    \n",
    "    modelo.add(Dense(n1, activation = 'relu', input_dim = 3))\n",
    "    modelo.add(Dense(n2, activation = 'linear'))\n",
    "    \n",
    "    sgd = optimizers.sgd(lr = learning_rate)\n",
    "    \n",
    "    modelo.compile(loss = 'mean_squared_error', optimizer = sgd, metrics = ['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "def escogerAccion(pModelo, pX):\n",
    "    num = np.random.random()\n",
    "    \n",
    "    if(num <= epsilon):\n",
    "        ind = np.random.randint(0,41)\n",
    "        return ind\n",
    "    else:\n",
    "        ind = np.argmax(pModelo.predict(pX.T))\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (30000 of 30000) |##################| Elapsed Time: 0:01:46 Time:  0:01:46\n"
     ]
    }
   ],
   "source": [
    "envName = 'Pendulum-v0'\n",
    "\n",
    "modelo = crearModelo(learning_rate = 0.001)\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.35\n",
    "epsilon = 1#Entre 0 y 1\n",
    "epsilonDecay = 0.999\n",
    "\n",
    "episodes = 30000\n",
    "ventana = 100\n",
    "contadores = np.zeros((1,41))\n",
    "\n",
    "rewards = np.zeros(int(episodes/ventana))\n",
    "\n",
    "env = gym.make(envName)\n",
    "observation = env.reset()\n",
    "\n",
    "x = observation\n",
    "x = np.reshape(x, (3,1))\n",
    "\n",
    "a = escogerAccion(modelo, x)\n",
    "\n",
    "suma = 0\n",
    "\n",
    "for i in progressbar.progressbar(range(episodes)):\n",
    "    \n",
    "    if(i > episodes-1000):\n",
    "        env.render()\n",
    "    \n",
    "    contadores[0,a] += 1\n",
    "    observation, reward, done, info = env.step([(a-20.0)/10.0])\n",
    "    \n",
    "    xp = observation\n",
    "    xp = np.reshape(xp, (3,1))\n",
    "    \n",
    "    r = reward #Por ahora\n",
    "    suma+=r\n",
    "    \n",
    "    if(i % ventana == 0):\n",
    "        rewards[int(i/ventana)] = suma/ventana\n",
    "        suma = 0\n",
    "    \n",
    "    qValores = modelo.predict(xp.T)\n",
    "    maxQvalor = np.max(qValores)\n",
    "    \n",
    "    #Aqui hay que calcular el maximo\n",
    "    target = r+gamma*maxQvalor\n",
    "    \n",
    "    y = modelo.predict(x.T)\n",
    "    \n",
    "    y[0,a] = target\n",
    "    \n",
    "    modelo.fit(x.T, y, verbose = 0)\n",
    "    \n",
    "    x = xp\n",
    "    a = escogerAccion(modelo, x)\n",
    "    \n",
    "    epsilon = epsilon*epsilonDecay\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5639679 , -0.49619234, -2.2416847 , -2.596084  , -0.7829679 ,\n",
       "         0.5874912 , -1.4217947 ,  2.0881338 , -4.469524  , -1.830524  ,\n",
       "        -0.41096663, -2.9731283 , -0.25903472,  0.5550725 ,  0.12394243,\n",
       "        -2.26297   ,  2.0600157 , -1.8249401 , -1.2904257 , -2.6254513 ,\n",
       "        -1.1714789 , -1.1079853 , -0.11576195, -0.67618024, -3.3362992 ,\n",
       "         0.11744235, -2.263774  ,  0.8623138 ,  2.1498265 , -0.2982781 ,\n",
       "        -1.4348308 ,  0.5167521 ,  0.5907148 , -1.3623844 ,  3.1878474 ,\n",
       "        -2.2512453 , -2.4177856 , -0.50901246, -2.4590547 , -1.579963  ,\n",
       "        -2.7358332 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx = np.array([15,5,1])\n",
    "testx = np.reshape(testx, (-1, 1))\n",
    "\n",
    "modelo.predict(np.transpose(testx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4995915609979532e-22"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx = np.array([1,1,0])\n",
    "testx = np.reshape(testx, (-1,1))\n",
    "escogerAccion(modelo, testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43077.,  3275.,  1421.,  1897.,  2223.,  1334.,  1636.,  1681.,\n",
       "         1472.,  1734.,  1859.,  1247.,  1852.,  1808.,  1403.,  1201.,\n",
       "         1504.,  1151.,  1100.,  1153.,  1195.,  1326.,  1099.,  1011.,\n",
       "         1867.,  1264.,  1137.,  1333.,  1205.,  1591.,  1261.,  1278.,\n",
       "         1084.,  1023.,   942.,  1228.,  1645.,  1029.,   987.,  1125.,\n",
       "         1342.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006736262610603238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9999**50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([10,22,50,-1,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = np.where(test == np.amax(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[10/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.504401749226135"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
